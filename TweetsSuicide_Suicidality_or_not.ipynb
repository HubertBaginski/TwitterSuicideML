{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import re\n",
    "import random\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import string\n",
    "from pattern.en import suggest\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import emoji\n",
    "import nlpaug.augmenter.word as naw\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# import of custom functions from local files\n",
    "from twitterFunctions.processing import processMe,process_token_fin,fix_emotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "file = \"./data/training_posts20201201_main_categories.tsv\"\n",
    "# read training data\n",
    "df = pd.read_csv(file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove exxagerations, we tested the impact, but ended up using the full data\n",
    "#df = df[df[\"ambiguous\"] != \"exaggeration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter relevant columns\n",
    "df = df[[\"notserious_unclear\", \"focus\", \"type\", \"about_suicide\", \"contents\"]]\n",
    "# rename class and text columns\n",
    "colNames = [\"notserious_unclear\", \"focus\", \"type\", \"class\", \"text\"]\n",
    "df.columns=colNames\n",
    "# mapping of 0's and 1's to correct and false\n",
    "di = {\n",
    "    1: \"correct\", # you can map it as you see fit\n",
    "    0: \"false\",\n",
    "}\n",
    "\n",
    "df[\"class\"].replace(di, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notserious_unclear</th>\n",
       "      <th>focus</th>\n",
       "      <th>type</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>correct</td>\n",
       "      <td>This is why joking about suicide on social med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>correct</td>\n",
       "      <td>Impulse - Ellen Hopkins-three teens meet in ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>correct</td>\n",
       "      <td>@User A darker subjectPeople who want to commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>correct</td>\n",
       "      <td>@User Dear God,Please continue working miracle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>correct</td>\n",
       "      <td>It Just Keeps Getting Worse: The Braxton Caner...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   notserious_unclear  focus  type    class  \\\n",
       "0                   0      1     4  correct   \n",
       "1                   0      1     4  correct   \n",
       "2                   0      1     4  correct   \n",
       "3                   0      1     4  correct   \n",
       "4                   0      1     4  correct   \n",
       "\n",
       "                                                text  \n",
       "0  This is why joking about suicide on social med...  \n",
       "1  Impulse - Ellen Hopkins-three teens meet in ho...  \n",
       "2  @User A darker subjectPeople who want to commi...  \n",
       "3  @User Dear God,Please continue working miracle...  \n",
       "4  It Just Keeps Getting Worse: The Braxton Caner...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from 'C:\\\\Users\\\\Hubert\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages\\\\matplotlib\\\\pyplot.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we print the class frequency distributions\n",
    "freq_combined = Counter(df[\"class\"].values)\n",
    "objects = []\n",
    "values = []\n",
    "for i in freq_combined.keys():\n",
    "    objects.append(i)\n",
    "for i in freq_combined.values():\n",
    "    values.append(i)\n",
    "    \n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Frequency ')\n",
    "plt.title('Class distribution tweets')\n",
    "plt.xticks(rotation=90)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run the preprocessing from the /twitterFunctions/processing.py\n",
    "# here we use 3 functions (each loops over the entire dataset, they have to run sequentailly)\n",
    "# if it takes too long one can make one function with all preprocessing - this recudes the runtime significantly\n",
    "temp = df.text.apply(processMe)\n",
    "# emojy mapping\n",
    "emojis = temp.apply(emoji.emojize)\n",
    "emojis = emojis.apply(fix_emotes)\n",
    "cleaned_text = emojis\n",
    "X_train_str = process_token_fin(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val and test split\n",
    "# stratify=df[\"class\"] ensures that the subsets contain a similar distributions as original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_str\n",
    "                    , df[\"class\"],test_size=0.2, random_state=1, stratify=df[\"class\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train\n",
    "                    , y_train,test_size=0.2, random_state=1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model name can be any transformers model (see https://huggingface.co/transformers/pretrained_models.html)\n",
    "modelName = \"bert-base-uncased\"\n",
    "#modelName = \"xlnet-base-cased\"\n",
    "# set default parameters for deep learning\n",
    "lr = 5e-5\n",
    "epoch = 3\n",
    "seed = 1\n",
    "maxlen = 512 # max 512 for BERT based models, for twitter we use visual anaylsis to reduces to ~100\n",
    "# the smaller the lenght the quicker the training\n",
    "classNames2 = y_train.unique()\n",
    "t = text.Transformer(modelName, \n",
    "                     maxlen = maxlen,  \n",
    "                         class_names = classNames2)\n",
    "# to ensure as much reproducibility as possbile we set all python and package seeds\n",
    "# BERT results still vary form run to run with fixed seeds and paramters due to internal segmentation\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# train function takes training texts, training labels, validation texts, and validation labels\n",
    "# STRONGLY RECOMMNED USING adding a checkpoint folder by settting checkpoint_folder, e.g.:\n",
    "# .., checkpoint_folder=\"D:/Huber/checkpoint/\"\n",
    "# this will save the paramters of each epoch. We can load the best one after training has finished\n",
    "def train_learner(X_train, y_train, X_val, y_val, \n",
    "                  lr, epoch, seed, text_length , checkpoint_folder=None): \n",
    "    classNames = classNames2\n",
    "    \n",
    "    set_seeds(seed)\n",
    "\n",
    "    t = text.Transformer(modelName, maxlen=text_length,  \n",
    "                         class_names = classNames2)\n",
    "    trn = t.preprocess_train(X_train,  y_train)\n",
    "    val = t.preprocess_test(X_val, y_val)\n",
    "    model2 = t.get_classifier()\n",
    "    learner = ktrain.get_learner(model2, train_data=trn, val_data=val, batch_size=32)\n",
    "    #learner.lr_find(max_epochs=5)\n",
    "    learner.fit_onecycle(lr, epoch, checkpoint_folder=checkpoint_folder)\n",
    "    return (learner, t, trn, val, model2)\n",
    "    #return learner.lr_plot()\n",
    "    \n",
    "# function that returns performance metrics and confusion matrix / only prints to console\n",
    "def get_performance(labels, pred, classNames, score=\"macro\"):\n",
    "    f1 = f1_score(labels, pred, average=score)\n",
    "    prec = precision_score(labels, pred, average=score)\n",
    "    rec  = recall_score(labels, pred, average=score)\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    print(\"; Prec: \", prec, \"; Rec: \", rec, \"F1: \", f1,   \"; Acc: \", acc)\n",
    "    print(confusion_matrix(labels, pred))\n",
    "    mat = confusion_matrix(labels, pred, labels=classNames)\n",
    "    return mat\n",
    "# function that uses a trained model and predicts on test set\n",
    "def predict_test(X_test, learner, t, trn):\n",
    "    trn=trn\n",
    "    predictor=ktrain.get_predictor(learner.model, preproc=t)\n",
    "    pred = predictor.predict(X_test)\n",
    "    return (np.squeeze(pred),predictor)\n",
    "# helper function used in synthetic data generation\n",
    "def filter_df(X,y,cond):\n",
    "    df_tr = pd.DataFrame({\"text\":X, \"class\":y})\n",
    "    df_minority = df_tr[df_tr[\"class\"]==cond]\n",
    "    df_minority\n",
    "    return df_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running grid search training TFIDF and SVM\n",
    "def run_SVM_CV(train_features, test_features, y_train, y_test, confusion = True, return_f1 = False, verbose = True):\n",
    "    metrics = np.zeros(4)\n",
    "    \n",
    "    # if u want to add SMOTE, just uncomment it in the pipline and paramters!\n",
    "    pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),  #CountVectorizer\n",
    " #  (\"smote\", SMOTE(random_state=1, n_jobs=-1)), #uncomment me if SMOTE\n",
    "    ('clf', SVC(random_state=1)),\n",
    "])\n",
    "\n",
    "    # here we can play around with the parameters\n",
    "    # for SVM params check https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    # for tfidf params check https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    \n",
    "    parameters = {\n",
    "      #  'vect__max_df': (0.5, 0.75, 1.0),\n",
    "      #  \"vect__stop_words\":[None, \"english\"],\n",
    "    'vect__max_features': (None, 10000, 25000, 50000),\n",
    "   'vect__ngram_range': ( (1,1), (1, 2) ),  # unigrams or bigrams\n",
    "    #    \"smote__k_neighbors\" : [5], # uncommnet me if SMOTE\n",
    "        'clf__C': np.arange(0.01, 1.01, 0.1),\n",
    "                        'clf__kernel' : [ \"rbf\", \"linear\"],\n",
    "                      'clf__class_weight': [\"balanced\"],\n",
    "                      \"clf__decision_function_shape\":[\"ovo\", \"ovr\"], }\n",
    "    scorer = sklearn.metrics.make_scorer(f1_score, average = 'macro')\n",
    "    # cv=5 is a 5-fold cross validation\n",
    "    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring=scorer)\n",
    "    # runs gridsearch with pipeline and paramters \n",
    "    gs_clf.fit(train_features, y_train)\n",
    "    print(gs_clf.best_params_)\n",
    "    test_pred = gs_clf.predict(test_features)\n",
    "    metrics+= [f1_score(y_test, test_pred, average=\"macro\"),\n",
    "                              precision_score(y_test, test_pred, average=\"macro\"),\n",
    "                              recall_score(y_test, test_pred, average=\"macro\"),\n",
    "                             accuracy_score(y_test, test_pred)\n",
    "                      ]\n",
    "    if verbose:\n",
    "            print('F1: {:.3f} | Pr: {:.3f} | Re: {:.3f} | Accuracy: {:.3f} \\n'.format(*metrics))\n",
    "    if return_f1:\n",
    "            return f1\n",
    "    if confusion:\n",
    "            print( confusion_matrix(y_test, test_pred))\n",
    "    return gs_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance: \n",
      "\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.51, 'clf__class_weight': 'balanced', 'clf__decision_function_shape': 'ovo', 'clf__kernel': 'linear', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2)}\n",
      "F1: 0.748 | Pr: 0.740 | Re: 0.760 | Accuracy: 0.801 \n",
      "\n",
      "[[323  60]\n",
      " [ 42  88]]\n",
      "Test Performance: \n",
      "\n",
      "; Prec:  0.7503340730665522 ; Rec:  0.7692904997818107 F1:  0.7585284817943778 ; Acc:  0.8096723868954758\n",
      "[[407  71]\n",
      " [ 51 112]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.5, 1.5]), <a list of 2 Text yticklabel objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x590.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Validation Performance: \\n\")\n",
    "model = run_SVM_CV(X_train, X_val, y_train, y_val)\n",
    "pred = model.predict(X_test)\n",
    "print(\"Test Performance: \\n\")\n",
    "mat = get_performance(y_test.values, pred, classNames2)\n",
    "\n",
    "labels = y_test.values\n",
    "index = columns = classNames2\n",
    "cm_df = pd.DataFrame(mat/np.sum(mat),columns,index)                      \n",
    "plt.figure(figsize=(10,8.2))  \n",
    "sns.heatmap(cm_df, annot=True, fmt=\".0%\"\n",
    "           )\n",
    "plt.yticks(rotation=0, fontsize=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading sklearn models\n",
    "\n",
    "for more information check out this article: \n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; Prec:  0.7503340730665522 ; Rec:  0.7692904997818107 F1:  0.7585284817943778 ; Acc:  0.8096723868954758\n",
      "[[407  71]\n",
      " [ 51 112]]\n"
     ]
    }
   ],
   "source": [
    "# loading the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict(X_test)\n",
    "mat = get_performance(y_test.values, result, classNames2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BERT model with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_learner = train_learner(\n",
    "       X_train, y_train.values, \n",
    "        X_val, y_val.values,\n",
    "        lr= 1.5e-5, epoch=1, seed=1, text_length=80 , # parameters for the training\n",
    "        checkpoint_folder=\"D:/models/test/\"   # add the path where the checkpoints should be saved\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the training has finished, you will see the training statistics. I usually choose the epoch with the highest validation accuracy (if two performances were similar I chose the one with the lower validation loss).\n",
    "\n",
    "If you used a checkpoint folder, you can load the model with the best performance rather than the last epoch. E.g. if you trained for 5 epochs, the default behaviour is that the model will use the last run, i.e. epoch 5. But if the performance of epoch 5 is worse than epoch 3, you can manually load the weights of epoch 3. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_learner[4].load_weights(\"D:/models/test/weights-01.hdf5\") # your path + weights-01.hdf5\n",
    "# where weights-*BEST_EPOCH*.hdf5\n",
    "\n",
    "learner_reloaded = ktrain.get_learner(original_learner[4], train_data=original_learner[2], val_data=original_learner[3], batch_size=2)\n",
    "\n",
    "model_ = learner_reloaded \n",
    "t_ = original_learner[1]\n",
    "trn_ = original_learner[2]\n",
    "\n",
    "# setting seeds (redundantly) before predicting validation set\n",
    "set_seeds(seed)\n",
    "# usually you only have to do this if you are interested in inter-class statistics of the valdiation set:D\n",
    "## PREDICT ON VALIDATION SET\n",
    "pred = predict_test(X_val, model_, \n",
    "                    t=t_,\n",
    "                    trn=trn_)\n",
    "predictor = pred[1]\n",
    "val = t_.preprocess_test(X_val, y_val.values)\n",
    "model_.validate(val_data= val )\n",
    "mat = get_performance(y_val.values, pred[0], classNames2)\n",
    "\n",
    "## PREDICT ON TEST SET\n",
    "set_seeds(seed)\n",
    "pred = predict_test(X_test, model_, \n",
    "                    t=t_,\n",
    "                    trn=trn_)\n",
    "predictor = pred[1]\n",
    "test = t_.preprocess_test(X_test, y_test.values)\n",
    "model_.validate(val_data= test , class_names = list(classNames2))\n",
    "mat = get_performance(y_test.values, pred[0], classNames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in binary classification we can get interclass precision by using the confusion matrix\n",
    "mat.diagonal()/mat.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a ktrain model\n",
    "\n",
    "If the test performance was good, and you want to save the actual model (not only the weights of the training) call **predictor.save( \"yourPath\")**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model / predictor\n",
    "predictor.save('D:/models/Twitter_about_suicide_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mat = get_performance(y_test.values, pred, classNames2)\n",
    "mat = [[21,5], [10,64]]\n",
    "labels = y_test.values\n",
    "capitalized = [i.capitalize() for i in classNames2]\n",
    "index = columns = capitalized\n",
    "cm_df = pd.DataFrame(mat/np.sum(mat),columns,index)                      \n",
    "plt.figure(figsize=(6,4.92))  \n",
    "sns.heatmap(cm_df, annot=True, fmt=\".0%\"\n",
    "           )\n",
    "plt.yticks(rotation=0, fontsize=12) \n",
    "plt.xlabel(\"Predicted class\", fontsize=20)\n",
    "plt.ylabel(\"Original class\", fontsize=20)\n",
    "plt.title(\"Confusion matrix test set, n=641\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two subset that only contain correct / false classes respectively\n",
    "major = filter_df(X_train,y_train.values,\"correct\")\n",
    "minor = filter_df(X_train,y_train.values,\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "set_seeds(seed)\n",
    "# sample number of minority observations from majoriy df\n",
    "AS = major.sample(n=int(minor.shape[0]),  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'false': 520, 'correct': 520})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = AS\n",
    "df_train = minor\n",
    "df_train = df_train.append(df_new, ignore_index = True)\n",
    "Counter(df_train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainign undersampled learner\n",
    "undersampled_learner = train_learner(\n",
    "        df_train.text.values, df_train[\"class\"].values,\n",
    "        X_val, y_val.values,\n",
    "       5e-5, 3, 2, 80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_learner[0].validate(class_names = list(classNames2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(seed)\n",
    "\n",
    "pred = predict_test(X_test, undersampled_learner[0], \n",
    "                    t=undersampled_learner[1],\n",
    "                    trn=undersampled_learner[2])\n",
    "\n",
    "test = undersampled_learner[1].preprocess_test(X_test, y_test.values)\n",
    "undersampled_learner[0].validate(val_data= test , class_names = list(classNames2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = get_performance(y_test.values, pred, classNames2)\n",
    "labels = y_test.values\n",
    "index = columns = classNames2\n",
    "cm_df = pd.DataFrame(mat/np.sum(mat),columns,index)                      \n",
    "plt.figure(figsize=(10,6))  \n",
    "sns.heatmap(cm_df, annot=True, fmt=\".0%\"\n",
    "           )\n",
    "plt.yticks(rotation=0) \n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.ylabel(\"Original class\")\n",
    "plt.title(\"Confusion matrix test set, n=641\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampled_learner[0].model.save_pretrained(\"D:/models/Suicidality_BERT_Undersampled_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing WordNet Synonym Lookup (get new df_train)\n",
    "\n",
    "This functions basically does three things.\n",
    "1. determine the majority class, get its frequency and ingore it.\n",
    "2. iterate through all remaining classes and:\n",
    "    use wordnet to replace aug_p words.\n",
    "    repeat until the class frequency == majority class frequency.\n",
    "   \n",
    "The result is a perfectly balanced training dataset, i.e. all classes have the same number of observations. Above you can find the code how to sample from a saved csv file. (see section @Use this when you have a synthetic training set already and want to continue training with synthetic training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting base frequency\n",
      "done, first class correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:50<00:00, 55.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, first class false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set augmentation probability\n",
    "aug_p = 0.3\n",
    "\n",
    "new_texts = []\n",
    "new_classes = []\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "freq_origin = Counter(y_train.values)\n",
    "majority_freq =  freq_origin.most_common()[0][1]\n",
    "majority_class = freq_origin.most_common()[0][0]\n",
    "seed = 1\n",
    "aug = naw.SynonymAug(aug_src='wordnet',\n",
    "                     aug_p=aug_p)\n",
    "for i in tqdm(freq_origin.most_common()):\n",
    "    df_tmp = filter_df(X_train, y_train.values, i[0])\n",
    "    if i[0] == majority_class:\n",
    "        print(\"getting base frequency\")\n",
    "    else:\n",
    "        num_lang = ( (majority_freq-i[1]) /  i[1] )\n",
    "        num_samples = num_lang * i[1]\n",
    "        if num_samples < i[1]:\n",
    "            AS = df_tmp[\"text\"].sample(n=int(num_samples),  random_state=seed)\n",
    "            TT = AS.values.tolist()\n",
    "            for tweet in TT:\n",
    "                augmented_text = aug.augment(tweet)\n",
    "                new_texts.append(augmented_text)\n",
    "                new_classes.append(i[0])\n",
    "        else:\n",
    "            for j in range(math.ceil(num_lang)):\n",
    "                if num_samples < i[1]:\n",
    "                    AS = df_tmp[\"text\"].sample(n=int(num_samples),  random_state=seed)\n",
    "                    TT = AS.values.tolist()\n",
    "                    for tweet in TT:\n",
    "                        augmented_text = aug.augment(tweet)\n",
    "                        new_texts.append(augmented_text)\n",
    "                        new_classes.append(i[0])\n",
    "                else:\n",
    "                    # translate entire set once\n",
    "                    AS = df_tmp[\"text\"]\n",
    "                    TT = AS.values.tolist()\n",
    "                    for tweet in TT:\n",
    "                        augmented_text = aug.augment(tweet)\n",
    "                        new_texts.append(augmented_text)\n",
    "                        new_classes.append(i[0])\n",
    "                    num_samples -= len(TT)        \n",
    "    seed+=1\n",
    "    print(\"done, first class\", i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving the dataset (uncomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'false': 1528, 'correct': 1528})\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.DataFrame({\n",
    "    \"text\": new_texts, \n",
    "    \"class\": new_classes\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    \"text\": X_train, \n",
    "    \"class\":  y_train.values\n",
    "})\n",
    "# appending the synthetic data to the orinignal traning data\n",
    "df_train = df_train.append(df_new, ignore_index = True)\n",
    "print(Counter(df_train[\"class\"]))\n",
    "\n",
    "# if you want to save or load use this: (uncommented ofc)\n",
    "#df_train.to_csv(\"40pc_oversampled.csv\", sep=\"|\")\n",
    "#df_train = pd.read_csv(\"40pc_oversampled.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPDB synonym lookup (get new df_train)\n",
    "\n",
    "This functions basically does three things.\n",
    "1. determine the majority class, get its frequency and ingore it.\n",
    "2. iterate through all remaining classes and:\n",
    "    use PPDB to replace aug_p words.\n",
    "    repeat until the class frequency == majority class frequency.\n",
    "   \n",
    "The result is a perfectly balanced training dataset, i.e. all classes have the same number of observations. Above you can find the code how to sample from a saved csv file. (see section @Use this when you have a synthetic training set already and want to continue training with synthetic training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting base frequency\n",
      "done, first class correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:50<00:00, 55.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, first class false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set augmentation probability\n",
    "aug_p = 0.2\n",
    "\n",
    "new_texts = []\n",
    "new_classes = []\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "freq_origin = Counter(y_train.values)\n",
    "majority_freq =  freq_origin.most_common()[0][1]\n",
    "majority_class = freq_origin.most_common()[0][0]\n",
    "seed = 1\n",
    "# download model here:\n",
    "    # http://nlpgrid.seas.upenn.edu/PPDB/eng/ppdb-2.0-tldr.gz\n",
    "# and change the model path:\n",
    "aug = naw.SynonymAug(aug_src='ppdb', model_path=\"D:\\models\\ppdb-2.0-tldr\\ppdb-2.0-tldr\",\n",
    "                     aug_p=aug_p)\n",
    "for i in tqdm(freq_origin.most_common()):\n",
    "    df_tmp = filter_df(X_train, y_train.values, i[0])\n",
    "    if i[0] == majority_class:\n",
    "        print(\"getting base frequency\")\n",
    "    else:\n",
    "        num_lang = ( (majority_freq-i[1]) /  i[1] )\n",
    "        num_samples = num_lang * i[1]\n",
    "        if num_samples < i[1]:\n",
    "            AS = df_tmp[\"text\"].sample(n=int(num_samples),  random_state=seed)\n",
    "            TT = AS.values.tolist()\n",
    "            for tweet in TT:\n",
    "                augmented_text = aug.augment(tweet)\n",
    "                new_texts.append(augmented_text)\n",
    "                new_classes.append(i[0])\n",
    "        else:\n",
    "            for j in range(math.ceil(num_lang)):\n",
    "                if num_samples < i[1]:\n",
    "                    AS = df_tmp[\"text\"].sample(n=int(num_samples),  random_state=seed)\n",
    "                    TT = AS.values.tolist()\n",
    "                    for tweet in TT:\n",
    "                        augmented_text = aug.augment(tweet)\n",
    "                        new_texts.append(augmented_text)\n",
    "                        new_classes.append(i[0])\n",
    "                else:\n",
    "                    # translate entire set once\n",
    "                    AS = df_tmp[\"text\"]\n",
    "                    TT = AS.values.tolist()\n",
    "                    for tweet in TT:\n",
    "                        augmented_text = aug.augment(tweet)\n",
    "                        new_texts.append(augmented_text)\n",
    "                        new_classes.append(i[0])\n",
    "                    num_samples -= len(TT)        \n",
    "    seed+=1\n",
    "    print(\"done, first class\", i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'false': 1528, 'correct': 1528})\n"
     ]
    }
   ],
   "source": [
    "# this combines the original dataframe with the synthetic one\n",
    "df_new = pd.DataFrame({\n",
    "    \"text\": new_texts, \n",
    "    \"class\": new_classes\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    \"text\": X_train, \n",
    "    \"class\":  y_train.values\n",
    "})\n",
    "\n",
    "df_train = df_train.append(df_new, ignore_index = True)\n",
    "\n",
    "print(Counter(df_train[\"class\"]))\n",
    "\n",
    "# saving the data (the translation takes a while, saving is recommended and can be skipped once u have the csv)\n",
    "# df_train.to_csv(\"40pc_oversampled.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this code chunk to modify the amount of synthetic data that will be added to the original data. \n",
    "\n",
    "By default it will be 100% syntehtic data, i.e. the final class distribtuion frequency will be the exact same for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'correct': 1528, 'false': 822})\n"
     ]
    }
   ],
   "source": [
    "# load syntehtic training data if saved\n",
    "#df_train = pd.read_csv(\"40pc_oversampled.csv\", sep=\"|\")\n",
    "\n",
    "# I always append the synthetic samples to the original training data\n",
    "# this ensures that df_train_en only contains the original training data\n",
    "df_train_en = df_train.iloc[0:len(X_train)]\n",
    "# determine the fraction of syntehtic data you want to add\n",
    "fc_ = 0.3\n",
    "# remove original training data from full training data (original + synthetic)\n",
    "a = df_train.drop(df_train_en.index, axis=0)\n",
    "# randomly sample fc_ fraction of synthtic data\n",
    "b = a.sample(random_state=1, frac=fc_)\n",
    "# add fc_ synthetic data to original\n",
    "c = df_train_en.append(b, ignore_index=True)\n",
    "# randomly shuffle final training data (original + fc_ syntetic)\n",
    "df_train = c.sample(frac=1)\n",
    "\n",
    "print( Counter(df_train[\"class\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code chunk that can be used to train with df_train (for all synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_learner = train_learner(\n",
    "        df_train.text.values, df_train[\"class\"].values, #X_train, y_train.values,\n",
    "        X_val, y_val.values,\n",
    "        lr= 1.5e-5, epoch=5, seed=1, text_length=80 , # parameters for the training\n",
    "        checkpoint_folder=\"D:/models/test/\"   # add the path where the checkpoints should be saved\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_learner[0].validate(class_names = list(classNames2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(seed)\n",
    "\n",
    "pred = predict_test(X_test, oversampled_learner[0], \n",
    "                    t=oversampled_learner[1],\n",
    "                    trn=oversampled_learner[2])\n",
    "\n",
    "test = oversampled_learner[1].preprocess_test(X_test, y_test.values)\n",
    "oversampled_learner[0].validate(val_data= test , class_names = list(classNames2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = get_performance(y_test.values, pred, classNames2)\n",
    "labels = y_test.values\n",
    "index = columns = classNames2\n",
    "cm_df = pd.DataFrame(mat,columns,index)                      \n",
    "plt.figure(figsize=(10,6))  \n",
    "sns.heatmap(cm_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"text\": X_test, \n",
    "    \"true_label\": y_test,\n",
    "    \"predicted_label\": pred\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below helps us visualize the models decisions\n",
    "\n",
    "I used this and below you can also see the code for the word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reloaded_predictor.explain(X_test[0],all_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_predictor.explain(X_test[0])\n",
    "#a = reloaded_predictor.explain(X_test[0],weights_df=True,all_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "doc = X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TextExplainer(random_state=42, n_samples=2500)\n",
    "_ = te.fit(doc, reloaded_predictor.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = te.explain_prediction(target_names = reloaded_predictor.c, targets= [pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5 import formatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "c = 0\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "for i in tqdm(X_test):\n",
    "    doc = i\n",
    "    te = TextExplainer(random_state=42, n_samples=1500)\n",
    "    _ = te.fit(doc, reloaded_predictor.predict_proba)\n",
    "    a = te.explain_prediction(target_names = reloaded_predictor.c, targets= [pred[c]])\n",
    "    df_weights = formatters.format_as_dataframe(a)\n",
    "    c+=1\n",
    "    c1.append(df_weights[0:1].values[0][1])\n",
    "    c2.append(df_weights[1:2].values[0][1])\n",
    "    c3.append(df_weights[2:3].values[0][1])\n",
    "    c4.append(df_weights[3:4].values[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"text\": X_test, \n",
    "    \"true_label\": y_test,\n",
    "    \"predicted_label\": pred,\n",
    "    \"1st feature\":c1,\n",
    "    \"2nd feature\":c2,\n",
    "    \"3rd feature\":c3,\n",
    "    \"4th feature\":c4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"test_withPredictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "Counter(df_test[\"predicted_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf = df_test[df_test[\"predicted_label\"]==\"Bereaved_negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dfdf[\"1st feature\"].values\n",
    "b = dfdf[\"2nd feature\"].values\n",
    "c = dfdf[\"3rd feature\"].values\n",
    "d = dfdf[\"4th feature\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.concatenate((a,b,c,d))\n",
    "tuples = [tuple(x) for x in a]\n",
    "fin = \"\"\n",
    "for i in A:\n",
    "    fin+=i+ \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400).generate(fin)\n",
    "plt.figure(dpi=800, facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "#plt.savefig(\"graph.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
